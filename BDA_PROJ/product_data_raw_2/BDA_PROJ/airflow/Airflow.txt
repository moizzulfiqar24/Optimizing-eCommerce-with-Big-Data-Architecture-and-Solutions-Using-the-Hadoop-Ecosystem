


find / -type f -name "airflow.db" 2>/dev/null
docker network connect bda_network airflow
docker exec -it --user root airflow /bin/bash

In kafka:
/opt/bitnami/kafka/bin/kafka-topics.sh --create --topic ProductTopic --bootstrap-server localhost:9092 --partitions 2 --replication-factor 1
/opt/bitnami/kafka/bin/kafka-topics.sh --create --topic OrderTopic --bootstrap-server localhost:9092 --partitions 2 --replication-factor 1
/opt/bitnami/kafka/bin/kafka-topics.sh --create --topic CustomerTopic --bootstrap-server localhost:9092 --partitions 2 --replication-factor 1

/opt/bitnami/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092


change the server.properties file in these directories
/opt/bitnami/kafka/config/kraft/server.properties
/opt/bitnami/kafka/config/server.properties

make sure it matches with this
############################# Server Basics #############################
broker.id=1

############################# Socket Server Settings #############################
listeners=PLAINTEXT://:9092
advertised.listeners=PLAINTEXT://kafka:9092
listener.security.protocol.map=PLAINTEXT:PLAINTEXT

num.network.threads=3
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600

############################# Log Basics #############################
log.dirs=/bitnami/kafka/data
num.partitions=1
num.recovery.threads.per.data.dir=1

############################# Internal Topic Settings #############################
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1

############################# Log Retention Policy #############################
log.retention.hours=168
log.retention.check.interval.ms=300000

############################# Zookeeper #############################
zookeeper.connect=zookeeperbda:2181

sasl.enabled.mechanisms=PLAIN,SCRAM-SHA-256,SCRAM-SHA-512


In namenode:
cd opt
tar -czvf hadoop-3.2.1.tar.gz hadoop-3.2.1/

Create the jar files here
echo $JAVA_HOME
export PATH=$JAVA_HOME/bin:$PATH
echo $HADOOP_HOME
export HADOOP_HOME=/opt/hadoop-3.2.1
export PATH=$HADOOP_HOME/bin:$PATH
mkdir -p classes
hadoop com.sun.tools.javac.Main -d classes /tmp/*.java      {if this is not working}


export HADOOP_CLASSPATH=$(find $HADOOP_HOME/share/hadoop -name "*.jar" | tr '\n' ':')
javac -d classes -cp $HADOOP_CLASSPATH CustomerDataMapper.java CustomerDataReducer.java CustomerDataDriver.java

jar -cvf customer-data-cleaner.jar -C classes .
jar -tf customer-data-cleaner.jar


tar -xzvf hbase.tar.gz 
echo "export HBASE_HOME=/hbase/hbase-1.2.6" >> ~/.bashrc
echo "export HADOOP_CLASSPATH=\$(find \$HADOOP_HOME/share/hadoop -name \"*.jar\" | tr '\n' ':'):\$HBASE_HOME/lib/*" >> ~/.bashrc
echo "export PATH=\$PATH:\$HADOOP_HOME/bin:\$HADOOP_HOME/sbin:\$HBASE_HOME/bin" >> ~/.bashrc
source ~/.bashrc

hdfs dfs -mkdir -p /user/airflow
hdfs dfs -chown airflow:supergroup /user/airflow
hdfs dfs -chmod 775 /user/airflow




In custom_airflow=
docker exec -it --user root custom_airflow /bin/bash
chmod 666 /var/run/docker.sock
docker exec -it custom_airflow /bin/bash


export FERNET_KEY=RP9QND5O48Du6-_W5lKvwI-NarPSZyZmTs9IfcMDork=
pip install kafka-python pyspark


mkdir /usr/local/airflow/hadoop
tar -xzvf hadoop-3.2.1.tar.gz

Install these libraries
pip show kafka-python pydoop

export HADOOP_HOME=/usr/local/airflow/hadoop/hadoop-3.2.1/
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

echo "export HADOOP_HOME=/usr/local/airflow/hadoop/hadoop-3.2.1/" >> ~/.bashrc
echo "export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin" >> ~/.bashrc
source ~/.bashrc

install java
apt-get install -y openjdk-11-jdk
mkdir -p /usr/share/man/man1
dpkg --configure -a
apt-get install -y openjdk-11-jdk


apt-get remove --purge openjdk-11-jdk openjdk-11-jre-headless ca-certificates-java {for removing}


tar -xzvf hbase-2.4.13-bin.tar.gz
export HBASE_HOME=/usr/local/airflow/hbase/hbase-2.4.13
export PATH=$PATH:$HBASE_HOME/bin
echo "export HBASE_HOME=/usr/local/airflow/hbase/hbase-2.4.13" >> ~/.bashrc
echo "export PATH=\$PATH:\$HBASE_HOME/bin" >> ~/.bashrc
source ~/.bashrc


place kafka scripts in airflow

hdfs dfs -mkdir /airflow
hdfs dfs -chown airflow:supergroup /airflow
hdfs dfs -chmod 775 /airflow


hadoop jar /usr/local/airflow/CustomerCleaner/customer-data-cleaner.jar CustomerDataDriver /airflow/customer_data_raw /airflow/customer_data_cleaned
hdfs dfs -ls /airflow/customer_data_cleaned
hdfs dfs -cat /airflow/customer_data_cleaned/part-00000

hadoop jar /usr/local/airflow/OrderCleaner/order-data-cleaner.jar OrderDataDriver /airflow/order_data_raw /airflow/order_data_cleaned /airflow/product_data_cleaned/part-r-00000

hdfs dfs -get /airflow/customer_data_cleaned/part-r-00000 /usr/local/airflow

hdfs dfs -mkdir /hfiles
hdfs dfs -chown hbase:supergroup /hfiles
hdfs dfs -chmod 775 /hfiles

echo "export HADOOP_CLASSPATH=$HBASE_HOME/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/yarn/*" >> ~/.bashrc
source ~/.bashrc

hadoop jar /usr/local/airflow/CustomerHFile/customer-hfile-generator.jar CustomerHFileDriver /airflow/customer_data_cleaned/part-r-00000 /hfiles/customer CustomerTable
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /hfiles/customer CustomerTable

ls /usr/local/airflow/hadoop/hadoop-3.2.1/share/hadoop/common/lib/ | grep guava
ls hbase/hbase-2.4.13/lib/ | grep guava

export HADOOP_CLASSPATH=$(find /usr/local/airflow/hadoop/hadoop-3.2.1/share/hadoop -name "*.jar" | tr '\n' ':'):/usr/local/airflow/hbase/hbase-2.4.13/lib/*
export HADOOP_CLASSPATH=$(find /usr/local/airflow/hbase/hbase-2.4.13/lib -name "*.jar" | tr '\n' ':'):$HADOOP_CLASSPATH
export HBASE_CLASSPATH=$(hbase classpath)
javac -d classes -cp $HADOOP_CLASSPATH CustomerHFileMapper.java CustomerHFileReducer.java CustomerHFileDriver.java  (if this not works)
javac -d classes -cp $HBASE_CLASSPATH CustomerHFileMapper.java CustomerHFileReducer.java CustomerHFileDriver.java
jar -cvf customer-hfile-generator.jar -C classes .
jar -tf customer-hfile-generator.jar


hdfs dfs -chown -R hbase:hbase /hfiles/customer
hdfs dfs -chown -R custom_airflow:custom_airflow /hfiles/customer


docker exec hadoop-namenode-bda hdfs dfs -chown -R hbase:hbase /hfiles
docker exec hadoop-namenode-bda hdfs dfs -chmod -R 775 /hfiles

docker exec hbasebda hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles hdfs://hadoop-namenode-bda:9000/hfiles/customer CustomerTable

docker exec hadoop-namenode-bda hdfs dfs -chown -R airflow:supergroup /hfiles

mkdir /usr/local/airflow/spark
tar -xzvf spark.tar.gz

export SPARK_HOME=/usr/local/airflow/spark
export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH
source ~/.bashrc

spark-submit --version

# spark-submit --master spark://spark-master-bda:7077 --deploy-mode client --jars /spark/jars/spark-hbase-connector_2.10-1.0.3.jar /usr/local/airflow/sparkanalysis/customer_analysis.py

spark-submit --master spark://spark-master-bda:7077 --deploy-mode client /usr/local/airflow/sparkanalysis/customerAnalysis.py
spark-submit --master spark://spark-master-bda:7077 --deploy-mode client --executor-memory 4G --total-executor-cores 4 --conf "spark.executor.heartbeatInterval=120s" --conf "spark.network.timeout=300s" /usr/local/airflow/sparkanalysis/customerAnalysis.py 


For hbase:
export HBASE_HOME=/hbase/hbase-1.2.6
export HADOOP_CLASSPATH=$(find $HADOOP_HOME/share/hadoop -name "*.jar" | tr '\n' ':'):$HBASE_HOME/lib/*
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HBASE_HOME/bin

javac -d classes -cp $HADOOP_CLASSPATH CustomerHFileMapper.java CustomerHFileReducer.java CustomerHFileDriver.java

hbase thrift start &



In airflow:

export FERNET_KEY=RP9QND5O48Du6-_W5lKvwI-NarPSZyZmTs9IfcMDork=
pip install kafka-python pyspark


mkdir /usr/local/airflow/hadoop
tar -xzvf hadoop-3.2.1.tar.gz

Install these libraries
pip show kafka-python pydoop

export HADOOP_HOME=/usr/local/airflow/hadoop/hadoop-3.2.1/
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

echo "export HADOOP_HOME=/usr/local/airflow/hadoop/hadoop-3.2.1/" >> ~/.bashrc
echo "export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin" >> ~/.bashrc
source ~/.bashrc

install java
apt-get install -y openjdk-11-jdk
mkdir -p /usr/share/man/man1
dpkg --configure -a
apt-get install -y openjdk-11-jdk


apt-get remove --purge openjdk-11-jdk openjdk-11-jre-headless ca-certificates-java {for removing}


tar -xzvf hbase-2.4.13-bin.tar.gz
export HBASE_HOME=/usr/local/airflow/hbase/hbase-2.4.13
export PATH=$PATH:$HBASE_HOME/bin
echo "export HBASE_HOME=/usr/local/airflow/hbase/hbase-2.4.13" >> ~/.bashrc
echo "export PATH=\$PATH:\$HBASE_HOME/bin" >> ~/.bashrc
source ~/.bashrc


place kafka scripts in airflow

hdfs dfs -mkdir /airflow
hdfs dfs -chown airflow:supergroup /airflow
hdfs dfs -chmod 775 /airflow


hadoop jar /usr/local/airflow/CustomerCleaner/customer-data-cleaner.jar CustomerDataDriver /airflow/customer_data_raw /airflow/customer_data_cleaned
hdfs dfs -ls /airflow/customer_data_cleaned
hdfs dfs -cat /airflow/customer_data_cleaned/part-00000

hadoop jar /usr/local/airflow/OrderCleaner/order-data-cleaner.jar OrderDataDriver /airflow/order_data_raw /airflow/order_data_cleaned /airflow/product_data_cleaned/part-r-00000

hdfs dfs -get /airflow/customer_data_cleaned/part-r-00000 /usr/local/airflow

hdfs dfs -mkdir /hfiles
hdfs dfs -chown hbase:supergroup /hfiles
hdfs dfs -chmod 775 /hfiles

echo "export HADOOP_CLASSPATH=$HBASE_HOME/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/yarn/*" >> ~/.bashrc
source ~/.bashrc

hadoop jar /usr/local/airflow/CustomerHFile/customer-hfile-generator.jar CustomerHFileDriver /airflow/customer_data_cleaned/part-r-00000 /hfiles/customer CustomerTable
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /hfiles/customer CustomerTable

ls /usr/local/airflow/hadoop/hadoop-3.2.1/share/hadoop/common/lib/ | grep guava
ls hbase/hbase-2.4.13/lib/ | grep guava

export HADOOP_CLASSPATH=$(find /usr/local/airflow/hadoop/hadoop-3.2.1/share/hadoop -name "*.jar" | tr '\n' ':'):/usr/local/airflow/hbase/hbase-2.4.13/lib/*
export HADOOP_CLASSPATH=$(find /usr/local/airflow/hbase/hbase-2.4.13/lib -name "*.jar" | tr '\n' ':'):$HADOOP_CLASSPATH
export HBASE_CLASSPATH=$(hbase classpath)
javac -d classes -cp $HADOOP_CLASSPATH CustomerHFileMapper.java CustomerHFileReducer.java CustomerHFileDriver.java  (if this not works)
javac -d classes -cp $HBASE_CLASSPATH CustomerHFileMapper.java CustomerHFileReducer.java CustomerHFileDriver.java
jar -cvf customer-hfile-generator.jar -C classes .
jar -tf customer-hfile-generator.jar



In dashboard:
streamlit run EDA.py --server.port 8501



In spark-master and worker=
env | grep SPARK_MASTER
tar -czvf spark.tar.gz spark/

In /opt
copy hbase.tar.gz
copy hadoop-3.2.1.tar.gz

tar -xvzf hbase.tar.gz
tar -xvzf hadoop-3.2.1.tar.gz

docker cp hbase-spark-1.0.0.7.1.2.6-1.jar spark-master-bda:/spark/jars/
docker cp spark-hbase-connector_2.10-1.0.3.jar spark-master-bda:/spark/jars/

export SPARK_HOME=/spark
export PATH=$PATH:$SPARK_HOME/bin
export HBASE_HOME=/opt/hbase-1.2.6
export PATH=$PATH:$HBASE_HOME/bin
export HADOOP_HOME=/opt/hadoop-3.2.1
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export SPARK_CLASSPATH=$SPARK_HOME/jars/*:$HBASE_HOME/lib/*:$HBASE_HOME/conf:/spark/jars/spark-hbase-connector_2.10-1.0.3.jar:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/*
export JAVA_HOME=/usr/lib/jvm/java-1.8-openjdk
export PATH=$JAVA_HOME/bin:$PATH
export SPARK_DIST_CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath)

source ~/.bashrc


echo $SPARK_CLASSPATH





In Zookeeper=
echo "ruok" | nc localhost 2181 if this command didn't return 'imok'

file to change /opt/bitnami/zookeeper/conf/zoo.cfg, in this file change to this '4lw.commands.whitelist=ruok,stat,conf,isro'






In Hbase:

tar -czvf hbase.tar.gz /opt/

/hbase-2.1.3/bin/start-hbase.sh


find the file 'hbase-2.1.3/conf/hbase-site.xml' and add the following properties (NO Need)
<configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://hadoop-namenode-bda:9000/hbase</value>
  </property>

  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
  </property>

  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>zookeeperbda</value>
  </property>

  <property>
    <name>hbase.zookeeper.property.clientPort</name>
    <value>2181</value>
  </property>

  <property>
    <name>zookeeper.znode.parent</name>
    <value>/hbase</value>
  </property>
  <property><name>hbase.regionserver.thrift.port</name><value>9090</value></property>
<property><name>hbase.regionserver.thrift.framed</name><value>false</value></property>
<property><name>hbase.regionserver.thrift.http</name><value>false</value></property>
</configuration>


mkdir /hadoop/
tar -xzvf hadoop-3.2.1.tar.gz

export HADOOP_HOME=/hadoop/hadoop-3.2.1/
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

echo "export HADOOP_HOME=/hadoop/hadoop-3.2.1/" >> ~/.bashrc
echo "export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin" >> ~/.bashrc
source ~/.bashrc

hdfs dfs -mkdir /hbase
hdfs dfs -chown hbase:supergroup /hbase
hdfs dfs -chmod 775 /hbase


# Create CustomerTable
create 'CustomerTable', 'info'

# Create ProductTable
create 'ProductTable', 'details', 'inventory'

# Create OrderTable
create 'OrderTable', 'info'


Extra commands:
hdfs dfs -rm -r -skipTrash <directory-path>



hdfs dfs -rm -r /airflow/product_data_cleaned/run-${timestampProduct}


hdfs dfs -rm -r /airflow/customer_data_cleaned/run-${timestampCustomer}

hdfs dfs -rm -r /airflow/order_data_cleaned/run-${timestampOrder}


hadoop jar customer-hfile-generator.jar CustomerHFileDriver /airflow/customer_data_cleaned/part-r-00000 /hfiles/customer CustomerTable
